{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lead Variant Effect dataset preparation\n",
    "\n",
    "The aim of this notebook is to collect the information about the effect of the credible set lead variants.\n",
    "**This includes**:\n",
    "\n",
    "- Addition of **Major population sample size** and **size of cases/controls** from _studyIndex_,\n",
    "- Addition of **VEP consequence score** derived annotations from _variantIndex_\n",
    "- Addition of **study specific major ancestry variant AF** (allele frequency)<a name=\"out of sample AF\"></a>[<sup>[1]</sup>](#cite_note-1) annotations from _variantIndex_\n",
    "- Calculation of **MAF (Minor Allele Frequency)** based on AF of the **credible set lead variants** derived from _studyLocus_\n",
    "- Calculation of **Variance Explained by lead variant**\n",
    "- Calculation of the **Rescaled estimated effect sizes** based on the trait class (dichotomous or continuous) and the MAF of the lead variant.\n",
    "\n",
    "<a name=\"cite_note-1\"></a>1. [^](#cite_ref-1) AF is derived from GnomAD v4.1 allele frequencies from joint Exome and WGS datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Data extraction and loading\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b style=\"font-size: 1.2em\">Downloading datasets</b><br><br>\n",
    "    <b>The analysis can be performed on the:</b>\n",
    "    <ul>\n",
    "        <li>2025.03 release (rsync from EBI FTP server)</li>\n",
    "        <li>2025.06 release (rsync from google cloud storage)</li>\n",
    "    </ul>\n",
    "    <I>This code chunk should be run only once to download the relevant datasets.</I>\n",
    "</div>\n",
    "\n",
    "Data for this analysis has to be downloaded from 3 datasets:\n",
    "\n",
    "- credible_set\n",
    "- variant\n",
    "- study\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.14\" 2025-01-21\n",
      "OpenJDK Runtime Environment Temurin-17.0.14+7 (build 17.0.14+7)\n",
      "OpenJDK 64-Bit Server VM Temurin-17.0.14+7 (build 17.0.14+7, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "# Ensure proper java version < 11\n",
    "!java -version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transfer starting: 27 files\n",
      "\n",
      "sent 16 bytes  received 1838 bytes  18540000 bytes/sec\n",
      "total size is 2578157705  speedup is 1390591.32\n",
      "Transfer starting: 3 files\n",
      "\n",
      "sent 16 bytes  received 158 bytes  1740000 bytes/sec\n",
      "total size is 93324727  speedup is 536345.92\n",
      "Transfer starting: 27 files\n",
      "\n",
      "sent 16 bytes  received 1836 bytes  18520000 bytes/sec\n",
      "total size is 3456816333  speedup is 1866530.49\n"
     ]
    }
   ],
   "source": [
    "# Download the release data from the Open Targets Platform 25.06 release\n",
    "!rsync -rpltvz --delete rsync.ebi.ac.uk::pub/databases/opentargets/platform/25.06/output/credible_set ../../data/.\n",
    "!rsync -rpltvz --delete rsync.ebi.ac.uk::pub/databases/opentargets/platform/25.06/output/study ../../data/.\n",
    "!rsync -rpltvz --delete rsync.ebi.ac.uk::pub/databases/opentargets/platform/25.06/output/variant ../../data/.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session setup\n",
    "\n",
    "- Create the sparkSession\n",
    "- Set all input/output paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"a6278e39-c5c0-4dd4-8c74-89c288b131d5\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"a6278e39-c5c0-4dd4-8c74-89c288b131d5\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"a6278e39-c5c0-4dd4-8c74-89c288b131d5\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gentropy.common.session import Session\n",
    "from gentropy.dataset.study_index import StudyIndex\n",
    "from gentropy.dataset.study_locus import StudyLocus\n",
    "from gentropy.dataset.variant_index import VariantIndex\n",
    "from pyspark.sql import functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/03 13:36:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/07/03 13:36:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/07/03 13:36:47 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/07/03 13:36:47 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "25/07/03 13:36:47 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "25/07/03 13:36:47 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "session = Session(extended_spark_conf={\"spark.driver.memory\": \"40G\"})\n",
    "variant_index_path = \"../../data/variant\"\n",
    "study_index_path = \"../../data/study\"\n",
    "credible_set_path = \"../../data/credible_set\"\n",
    "output_dataset_path = \"../../data/lead_variant_effect\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://mib117351s.internal.sanger.ac.uk:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>gentropy</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x309c13890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building temporary dataset\n",
    "\n",
    "The temporary dataset needs to be build from the _studyIndex_, _studyLocus_ and _variantIndex_ datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = VariantIndex.from_parquet(session, variant_index_path)\n",
    "si = StudyIndex.from_parquet(session, study_index_path)\n",
    "cs = StudyLocus.from_parquet(session, credible_set_path)\n",
    "\n",
    "\n",
    "_cs = cs.df.select(\n",
    "    f.col(\"studyId\"),\n",
    "    f.col(\"studyLocusId\"),\n",
    "    f.col(\"variantId\"),\n",
    "    f.col(\"beta\"),\n",
    "    f.col(\"zScore\"),\n",
    "    f.col(\"pValueMantissa\"),\n",
    "    f.col(\"pValueExponent\"),\n",
    "    f.col(\"standardError\"),\n",
    "    f.col(\"finemappingMethod\"),\n",
    "    f.col(\"studyType\"),\n",
    "    f.col(\"locus\"),\n",
    "    f.col(\"isTransQtl\"),\n",
    ")\n",
    "_si = si.df.select(\n",
    "    f.col(\"studyId\"),\n",
    "    f.col(\"nSamples\"),\n",
    "    f.col(\"nControls\"),\n",
    "    f.col(\"nCases\"),\n",
    "    f.col(\"geneId\"),  # for molqtl traits\n",
    "    f.col(\"traitFromSourceMappedIds\"),\n",
    "    f.col(\"ldPopulationStructure\"),\n",
    "    f.col(\"traitFromSource\"),\n",
    "    f.col(\"traitFromSourceMappedIds\"),\n",
    ")\n",
    "\n",
    "_vi = vi.df.select(\n",
    "    f.col(\"variantId\"),\n",
    "    f.col(\"allelefrequencies\"),\n",
    "    f.col(\"variantEffect\"),\n",
    "    f.col(\"transcriptConsequences\"),\n",
    "    f.col(\"chromosome\"),\n",
    "    f.col(\"position\"),\n",
    "    f.col(\"referenceAllele\"),\n",
    "    f.col(\"alternateAllele\"),\n",
    ")\n",
    "\n",
    "dataset = _cs.join(_si, how=\"left\", on=\"studyId\").join(_vi, how=\"left\", on=\"variantId\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAF Calculation\n",
    "\n",
    "To add the MAF (Minor Allele Frequency) to the dataset we need to extract the major ancestry from _studyIndex_ and use it to extract the relevant allele frequency from the _variantIndex_ dataset.\n",
    "\n",
    "The MAF is calculated as follows:\n",
    "\n",
    "<ol>\n",
    "    <li>Extract the major ancestry from the <code>studyIndex</code> dataset.</li>\n",
    "    <ol>\n",
    "        <li>In case there are multiple ancestries that match the <code>relativeSampleSize</code>, and one of them is <code>NFE</code>, use <code>NFE</code> as the major ancestry.</li>\n",
    "        <li>In case there are multiple ancestries that match the <code>relativeSampleSize</code> and none of them is <code>NFE</code>, use the first ancestry in the list as the major ancestry.</li>\n",
    "        <li>If there is no ancestry in the list, use <code>NFE</code> as the major ancestry, assign the <code>relativeSampleSize</code> to 0.0</li>\n",
    "    </ol>\n",
    "    <li>Extract the allele frequency for the major ancestry from the <code>variantIndex</code> dataset.</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manuscript_methods.ld_populations import LDPopulationName, LDPopulationStructure\n",
    "from manuscript_methods.maf import AlleleFrequencies\n",
    "\n",
    "ld_pop = LDPopulationStructure(f.col(\"ldPopulationStructure\"))\n",
    "major_ld_pop = ld_pop.major_population(default_major_pop=LDPopulationName.NFE)\n",
    "major_ld_maf = AlleleFrequencies(f.col(\"alleleFrequencies\")).ld_population_maf(major_ld_pop.ld_population)\n",
    "major_ld_af = AlleleFrequencies(f.col(\"alleleFrequencies\")).ld_population_af(major_ld_pop.ld_population)\n",
    "\n",
    "dataset = dataset.withColumns(\n",
    "    {\n",
    "        \"majorLdPopulation\": major_ld_pop.col,\n",
    "        \"majorLdPopulationMaf\": major_ld_maf.col,\n",
    "        \"majorLdPopulationAf\": major_ld_af.col,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# dataset.select(\"majorLdPopulation\", \"majorLdPopulationMaf\", \"majorLdPopulationAf\").show(5, truncate=False)\n",
    "# dataset.select(\"majorLdPopulation\", \"majorLdPopulationMaf\", \"majorLdPopulationAf\").printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phenotypic variance explained by lead variant (Approximation)\n",
    "\n",
    "The code below is used to calculate the PVE (Phenotypic Variance Explained) by the lead variant in the credible set.\n",
    "\n",
    "The variance explained follows the simplified formula\n",
    "\n",
    "${variance\\;explained}=\\chi^2 / n $\n",
    "\n",
    "- The $\\chi^2$ is calculated as **Inverse survival function** by using `scipy.stats.isf` function from lead variant $pValue$ (depicted as `pValueMantissa` and `pValueExponent`).\n",
    "- The $n$ parameter is the number of samples derived from GWAS study description.\n",
    "\n",
    "- In case where the `pValueExponent < 300` to avoid floating point errors we estimate $\\chi^2$ statistic with $-log_{10}(pValue)$\n",
    "- The $variance\\;explained$ can be only calculated where the $n > 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ss60/Projects/Gentropy-manuscript/.venv/lib/python3.11/site-packages/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n"
     ]
    }
   ],
   "source": [
    "from manuscript_methods.variant_statistics import PValueComponents, VariantStatistics\n",
    "\n",
    "pval_components = PValueComponents(p_value_mantissa=f.col(\"pValueMantissa\"), p_value_exponent=f.col(\"pValueExponent\"))\n",
    "n_samples = f.col(\"nSamples\")\n",
    "variant_stats = VariantStatistics.compute(pval_components, n_samples)\n",
    "\n",
    "dataset = dataset.withColumns(\n",
    "    {\n",
    "        \"variantStatistics\": variant_stats.col,\n",
    "    }\n",
    ")\n",
    "# dataset.select(\"variantStatistics\").show(5, truncate=False)\n",
    "# dataset.select(\"variantStatistics\").printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study statistics\n",
    "\n",
    "The code below is used to combine and classify the cohort statistics from the _studyIndex_ dataset.\n",
    "\n",
    "This includes:\n",
    "\n",
    "- n_cases\n",
    "- n_controls\n",
    "- n_samples\n",
    "- study_type\n",
    "- trait\n",
    "- trait_ids\n",
    "- gene_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manuscript_methods.study_statistics import StudyStatistics\n",
    "\n",
    "cohort_stat = StudyStatistics.compute(\n",
    "    n_samples=f.col(\"nSamples\"),\n",
    "    n_cases=f.col(\"nCases\"),\n",
    "    n_controls=f.col(\"nControls\"),\n",
    "    trait=f.col(\"traitFromSource\"),\n",
    "    study_type=f.col(\"studyType\"),\n",
    "    is_trans_pqtl=f.col(\"isTransQtl\"),\n",
    "    gene_id=f.col(\"geneId\"),\n",
    ")\n",
    "\n",
    "dataset = dataset.withColumns({\"studyStatistics\": cohort_stat.col})\n",
    "# dataset.select(\"studyStatistics\").show(5, truncate=False)\n",
    "# dataset.select(\"studyStatistics\").printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescaling of the marginal effect size\n",
    "\n",
    "Rescaling of marginal effect size to the original value from the standardised marginal effect size is done via two formulas depending on trait being **quantitative** or **binary**\n",
    "\n",
    "Estimation of the trait type is done on the basis of availability of reported `nCases` and `nControls` fields in the study description.\n",
    "\n",
    "- In case both fields are non empty and non zero we assume _binary trait_\n",
    "- In case cases are zero or are not reported we assume _quantitative trait_\n",
    "\n",
    "In both cases we estimate the marginal effect size $estimated\\;\\beta$ with following formula\n",
    "$$estimated\\;\\beta = zscore \\cdot se$$\n",
    "\n",
    "Where\n",
    "\n",
    "- $zscore = \\frac{\\beta}{|{\\beta}|} \\cdot \\sqrt{\\chi^2}$\n",
    "- $se$ depends on the trait type\n",
    "- $\\beta$ - _standardised beta reported from in the summary statistics_\n",
    "\n",
    "In case when $\\beta$ was not reported we assumed the $\\frac{\\beta}{|{\\beta}|}$ to be equal to 1\n",
    "\n",
    "#### Binary trait marginal effect size estimation\n",
    "\n",
    "$$se = \\frac{1}{\\sqrt{(varG \\cdot prev \\cdot (1 - prev))}}$$\n",
    "\n",
    "- $varG = 2 \\cdot f \\cdot (1 - f)$ - _component of genetic variance_ - the original is $var_{G} = 2\\beta^2f(1 - f)$\n",
    "- $f$ - _Minor Allele Frequency of lead variant_\n",
    "- $prev = \\frac{nCases}{nSamples}$ - _Trait prevelance_\n",
    "\n",
    "#### Quantative trait marginal effect size estimation\n",
    "\n",
    "$$se = \\frac{1}{\\sqrt{varG}}$$\n",
    "\n",
    "- $varG = 2 \\cdot f \\cdot (1 - f)$\n",
    "- $f$ - _Minor Allele Frequency of lead variant_\n",
    "\n",
    "The $\\chi^2$ was esteimated as described in `variance Explained` calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manuscript_methods.maf import MinorAlleleFrequency, PopulationFrequency\n",
    "from manuscript_methods.rescaled_beta import RescaledStatistics\n",
    "\n",
    "rescaled_stats = RescaledStatistics.compute(\n",
    "    beta=f.col(\"beta\"),\n",
    "    chi2_stat=VariantStatistics(f.col(\"variantStatistics\")).chi2_stat,\n",
    "    trait_class=StudyStatistics(f.col(\"studyStatistics\")).trait_class,\n",
    "    af=PopulationFrequency(f.col(\"majorLdPopulationAf\")).allele_frequency,\n",
    "    maf=MinorAlleleFrequency(f.col(\"majorLdPopulationMaf\")).value,\n",
    "    n_cases=StudyStatistics(f.col(\"studyStatistics\")).n_cases,\n",
    "    n_samples=StudyStatistics(f.col(\"studyStatistics\")).n_samples,\n",
    ")\n",
    "\n",
    "dataset = dataset.withColumns({\"rescaledStatistics\": rescaled_stats.col})\n",
    "\n",
    "# dataset.select(\"rescaledStatistics\").show(5, truncate=False)\n",
    "# dataset.select(\"rescaledStatistics\").printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VEP consequence extraction\n",
    "\n",
    "To extract the VEP annotations from the _variantIndex_ dataset we need to:\n",
    "\n",
    "- for GWAS lead variants extract the VEP annotation with most severe consequence\n",
    "- for QTL lead variants extract the VEP annotation that is linked to the `geneId` defined in the _studyIndex_ dataset in case\n",
    "  the `geneId` is found in transcript annotations (**in-gene effect**), otherwise use the most severe consequence annotation `(**out-gene effect**).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manuscript_methods.study_statistics import StudyStatistics\n",
    "from manuscript_methods.tc import LeadVariantConsequences, TranscriptConsequences\n",
    "\n",
    "tc = TranscriptConsequences(f.col(\"transcriptConsequences\"))\n",
    "sstats = StudyStatistics(f.col(\"studyStatistics\"))\n",
    "lc = LeadVariantConsequences.compute(tc, sstats)\n",
    "\n",
    "\n",
    "dataset = dataset.withColumn(lc.name, lc.col)\n",
    "# dataset.select(lc.name).show(5, truncate=False)\n",
    "# dataset.select(lc.name).printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locus statistics\n",
    "\n",
    "The locus statistics gets:\n",
    "\n",
    "- Posterior Probability of the lead variant\n",
    "- Locus length\n",
    "- Locus size\n",
    "- Locus start - (start of the first variant in the locus)\n",
    "- Locus end - (end of the last variant in the locus)\n",
    "\n",
    "All statistics are derived from the **studyLocus** dataset `locus` object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manuscript_methods.locus_statistics import LocusStatistics\n",
    "\n",
    "ls = LocusStatistics.compute(locus=f.col(\"locus\"), lead_variant=f.col(\"variantId\"))\n",
    "dataset = dataset.withColumn(ls.name, ls.col)\n",
    "# dataset.select(ls.name).show(5, truncate=False)\n",
    "# dataset.select(f\"{ls.name}.*\").printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variant type for interval joins\n",
    "\n",
    "Compute the variant type and effective length of Indels to use them downstream for interval joins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from manuscript_methods.variant_type import Variant\n",
    "\n",
    "dataset = dataset.withColumn(\n",
    "    \"variant\",\n",
    "    Variant.compute(f.col(\"chromosome\"), f.col(\"position\"), f.col(\"referenceAllele\"), f.col(\"alternateAllele\")).col,\n",
    ")\n",
    "\n",
    "# dataset.select(\"variant\").show(5, truncate=False)\n",
    "# dataset.select(\"variant\").printSchema()\n",
    "# # Show the number of lead variants per type\n",
    "# dataset.select(\"variant.*\").groupby(\"type\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataset contract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.select(\n",
    "    f.col(\"variantId\"),\n",
    "    f.col(\"variant\"),\n",
    "    f.col(\"studyLocusId\"),\n",
    "    f.col(\"studyId\"),\n",
    "    f.col(\"geneId\"),\n",
    "    f.col(\"beta\").alias(\"originalBeta\"),\n",
    "    f.col(\"standardError\").alias(\"originalStandardError\"),\n",
    "    f.col(\"locusStatistics\"),\n",
    "    f.col(\"finemappingMethod\"),\n",
    "    f.col(\"isTransQtl\"),\n",
    "    f.col(\"variantEffect\"),\n",
    "    f.col(\"majorLdPopulation\"),\n",
    "    f.col(\"majorLdPopulationMaf\"),\n",
    "    f.col(\"majorLdPopulationAf\"),\n",
    "    f.col(\"variantStatistics\"),\n",
    "    f.col(\"studyStatistics\"),\n",
    "    f.col(\"rescaledStatistics\"),\n",
    "    f.col(\"leadVariantConsequence\"),\n",
    "    f.col(\"traitFromSourceMappedIds\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dataset to parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/02 14:06:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataset.repartition(50).write.mode(\"overwrite\").parquet(output_dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../../src/manuscript_methods/schemas/lead_variant_effect.json\", \"w\") as fp:\n",
    "    json.dump(\n",
    "        json.loads(dataset.schema.json()),\n",
    "        fp,\n",
    "        indent=2,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 115:>                                                        (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with 2833758 rows saved to ../../data/lead-maf-vep\n",
      "1725150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset with {dataset.count()} rows saved to {output_dataset_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gentropy-manuscript",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
